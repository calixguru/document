{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca40337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import re\n",
    "\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "import csv\n",
    "from io import StringIO, BytesIO\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "from urllib.parse import urlparse\n",
    "import csv\n",
    "from io import StringIO, BytesIO\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5c0808",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://www.betexplorer.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7930f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download(pk):\n",
    "    def get_first_fixture_block(league_href):\n",
    "        fixtures_url = urljoin(BASE, league_href.rstrip(\"/\") + \"/fixtures/\")\n",
    "        html = requests.get(fixtures_url, timeout=15).text\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        table = soup.select_one(\"#js-leaguefixtures-all table.table-main\")\n",
    "        if not table:\n",
    "            return []\n",
    "\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        blocks = []          # will hold [block1, block2]\n",
    "        current_block = []   # temp holder for matches\n",
    "        started = False      # once we meet first round header\n",
    "\n",
    "        for tr in rows:\n",
    "            # Detect header row (\"12. Round\", \"13. Round\", etc)\n",
    "            if tr.find(\"th\"):\n",
    "                if not started:\n",
    "                    # First header found → start first block\n",
    "                    started = True\n",
    "                    # start new block\n",
    "                    if current_block:\n",
    "                        blocks.append(current_block)\n",
    "                    current_block = []\n",
    "                    continue\n",
    "                else:\n",
    "                    # header after starting → new block\n",
    "                    if current_block:\n",
    "                        blocks.append(current_block)\n",
    "\n",
    "                    if len(blocks) == 2:\n",
    "                        break  # we only care about 2 blocks\n",
    "\n",
    "                    current_block = []\n",
    "                    continue\n",
    "\n",
    "            if not started:\n",
    "                continue\n",
    "\n",
    "            # extract datetime\n",
    "            dt_td = tr.find(\"td\", class_=\"table-main__datetime\")\n",
    "            if not dt_td:\n",
    "                continue\n",
    "            datetime_text = dt_td.get_text(strip=True)\n",
    "\n",
    "            # extract match link\n",
    "            link_td = tr.find(\"a\", class_=\"in-match\")\n",
    "            if not link_td:\n",
    "                continue\n",
    "            match_href = link_td[\"href\"]\n",
    "            match_url = urljoin(BASE, match_href)\n",
    "\n",
    "            # extract teams\n",
    "            spans = link_td.find_all(\"span\")\n",
    "            if len(spans) != 2:\n",
    "                continue\n",
    "            home = spans[0].get_text(strip=True)\n",
    "            away = spans[1].get_text(strip=True)\n",
    "\n",
    "            current_block.append({\n",
    "                \"datetime\": datetime_text,\n",
    "                \"home\": home,\n",
    "                \"away\": away,\n",
    "                \"match_link\": match_url\n",
    "            })\n",
    "\n",
    "        # append last collected block\n",
    "        if current_block:\n",
    "            blocks.append(current_block)\n",
    "\n",
    "        # ---- DECISION LOGIC ----\n",
    "        if not blocks:\n",
    "            return []\n",
    "\n",
    "        first_block = blocks[0] if len(blocks) > 0 else []\n",
    "        second_block = blocks[1] if len(blocks) > 1 else []\n",
    "\n",
    "        if len(first_block) > 4:\n",
    "            return first_block  # return first block only\n",
    "        else:\n",
    "            return second_block  # return second block instead\n",
    "\n",
    "\n",
    "    leagues = [\n",
    "        {\"name\": 'League', \"href\": pk},\n",
    "    ]\n",
    "\n",
    "    linkss = []\n",
    "\n",
    "    for lg in leagues:\n",
    "        block = get_first_fixture_block(lg[\"href\"])\n",
    "        for m in block:\n",
    "            linkss.append({\n",
    "            \"date\": m[\"datetime\"],\n",
    "            \"home_team\": m[\"home\"],\n",
    "            \"away_team\": m[\"away\"],\n",
    "            \"link\": m[\"match_link\"]\n",
    "        })\n",
    "\n",
    "\n",
    "    def compute_fixture_stats(home_team, away_team, home_results, away_results, date):\n",
    "        \"\"\"\n",
    "        home_results and away_results must already be:\n",
    "        - filtered to same competition\n",
    "        - sorted from most recent → oldest\n",
    "        \"\"\"\n",
    "\n",
    "        def extract_home_count(matches):\n",
    "            return sum(1 for m in matches if m[\"is_home\"] is True)\n",
    "\n",
    "        def extract_away_count(matches):\n",
    "            return sum(1 for m in matches if m[\"is_home\"] is False)\n",
    "\n",
    "        # -------------------------------\n",
    "        # TRY LAST 6 MATCHES (STRICT 3-3)\n",
    "        # -------------------------------\n",
    "        if len(home_results) >= 6 and len(away_results) >= 6:\n",
    "            last6_home = home_results[:6]\n",
    "            last6_away = away_results[:6]\n",
    "\n",
    "            H_home_count = extract_home_count(last6_home)\n",
    "            A_away_count = extract_away_count(last6_away)\n",
    "\n",
    "            if H_home_count == 3 and A_away_count == 3:\n",
    "                sorting_balance = 6\n",
    "                home_selected = [m for m in last6_home if m[\"is_home\"]]\n",
    "                away_selected = [m for m in last6_away if not m[\"is_home\"]]\n",
    "            else:\n",
    "                sorting_balance = None\n",
    "                home_selected = None\n",
    "                away_selected = None\n",
    "        else:\n",
    "            sorting_balance = None\n",
    "            home_selected = None\n",
    "            away_selected = None\n",
    "\n",
    "        # --------------------------------------\n",
    "        # TRY LAST 7 MATCHES IF LAST 6 FAILED\n",
    "        # --------------------------------------\n",
    "        if sorting_balance is None:\n",
    "            if len(home_results) >= 7 and len(away_results) >= 7:\n",
    "                last7_home = home_results[:7]\n",
    "                last7_away = away_results[:7]\n",
    "\n",
    "                H_count = extract_home_count(last7_home)\n",
    "                A_count = extract_away_count(last7_away)\n",
    "\n",
    "                # STRICT RULE: must be equal AND must be 3 or 4\n",
    "                if H_count == A_count and H_count in (3, 4):\n",
    "                    sorting_balance = 7\n",
    "                    home_selected = [m for m in last7_home if m[\"is_home\"]]\n",
    "                    away_selected = [m for m in last7_away if not m[\"is_home\"]]\n",
    "                else:\n",
    "                    return None  # reject completely\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        # At this point sorting balance is validated strictly.\n",
    "\n",
    "        # ------------------ COMPUTATION HELPERS ------------------\n",
    "\n",
    "        def avg_goals(matches):\n",
    "            gf = sum(m[\"goals_for\"] for m in matches) / len(matches)\n",
    "            ga = sum(m[\"goals_against\"] for m in matches) / len(matches)\n",
    "            return gf, ga\n",
    "\n",
    "        def count_scored_2plus(matches):\n",
    "            return sum(1 for m in matches if m[\"goals_for\"] >= 2) / len(matches)\n",
    "\n",
    "        def count_conceded_2plus(matches):\n",
    "            return sum(1 for m in matches if m[\"goals_against\"] >= 2) / len(matches)\n",
    "\n",
    "        def outcomes(matches):\n",
    "            wins = sum(1 for m in matches if m[\"goals_for\"] > m[\"goals_against\"])\n",
    "            draws = sum(1 for m in matches if m[\"goals_for\"] == m[\"goals_against\"])\n",
    "            losses = sum(1 for m in matches if m[\"goals_for\"] < m[\"goals_against\"])\n",
    "            total = len(matches)\n",
    "            return wins/total, draws/total, losses/total\n",
    "\n",
    "        # ------------------ HOME TEAM STATS ------------------\n",
    "        HTS, HTC = avg_goals(home_selected)\n",
    "        HS_plus = count_scored_2plus(home_selected)\n",
    "        HC_plus = count_conceded_2plus(home_selected)\n",
    "        PHW, PHX, PHL = outcomes(home_selected)\n",
    "\n",
    "        # ------------------ AWAY TEAM STATS ------------------\n",
    "        ATS, ATC = avg_goals(away_selected)\n",
    "        AS_plus = count_scored_2plus(away_selected)\n",
    "        AC_plus = count_conceded_2plus(away_selected)\n",
    "        PAW, PAX, PAL = outcomes(away_selected)\n",
    "\n",
    "        return {\n",
    "            \"sorting_balance\": sorting_balance,\n",
    "            \"home_team\": home_team,\n",
    "            \"away_team\": away_team,\n",
    "            \"date\": date,\n",
    "            \"stats\": {\n",
    "                \"HTS\": HTS, \"HTC\": HTC,\n",
    "                \"ATS\": ATS, \"ATC\": ATC,\n",
    "\n",
    "                \"HS_plus_1\": HS_plus,\n",
    "                \"HS-1\": 1 - HS_plus,\n",
    "\n",
    "                \"HC_plus_1\": HC_plus,\n",
    "                \"HC-1\": 1 - HC_plus,\n",
    "\n",
    "                \"AS_plus_1\": AS_plus,\n",
    "                \"AS-1\": 1 - AS_plus,\n",
    "\n",
    "                \"AC_plus_1\": AC_plus,\n",
    "                \"AC-1\": 1 - AC_plus,\n",
    "\n",
    "                \"PHW\": PHW, \"PHX\": PHX, \"PHL\": PHL,\n",
    "                \"PAW\": PAW, \"PAX\": PAX, \"PAL\": PAL\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 1) Extract team links and competition link from match page\n",
    "    # -----------------------------------------------------------\n",
    "    def get_team_links_and_comp(match_url):\n",
    "\n",
    "        html = requests.get(match_url, timeout=15).text\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        # team links are the first two <a> inside .list-details\n",
    "        team_tags = soup.select(\"ul.list-details li.list-details__item a\")\n",
    "\n",
    "        if len(team_tags) < 2:\n",
    "            raise ValueError(\"Could not find 2 team links in match page\")\n",
    "\n",
    "        home_href = team_tags[0][\"href\"]\n",
    "        away_href = team_tags[1][\"href\"]\n",
    "\n",
    "        home_url = urljoin(BASE, home_href)\n",
    "        away_url = urljoin(BASE, away_href)\n",
    "\n",
    "        # competition shown in header breadcrumb\n",
    "        comp_tag = soup.select_one(\"ul.breadcrumb__ul li:nth-child(4) a\")\n",
    "        if not comp_tag:\n",
    "            raise ValueError(\"Could not detect competition link\")\n",
    "\n",
    "        comp_href = comp_tag[\"href\"]  # e.g. /football/england/premier-league/\n",
    "\n",
    "        return home_url, away_url, comp_href\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 2) Fetch team /results/ page\n",
    "    # -----------------------------------------------------------\n",
    "    def get_team_results_page(team_url):\n",
    "        results_url = team_url.rstrip(\"/\") + \"/results/\"\n",
    "\n",
    "        html = requests.get(results_url, timeout=15).text\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        return soup\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 3) Extract rows belonging ONLY to this competition\n",
    "    # -----------------------------------------------------------\n",
    "    def get_results_for_competition(soup, competition_href):\n",
    "        results = []\n",
    "        in_target_comp = False\n",
    "\n",
    "        # normalize competition\n",
    "        competition_href = competition_href.rstrip(\"/\")\n",
    "\n",
    "        # each block starts with <th class=\"table-main__tournament\">\n",
    "        for tr in soup.select(\"table.teaminfo tr\"):\n",
    "\n",
    "            # --- COMPETITION HEADER DETECTION ---\n",
    "            header = tr.find(\"a\", class_=\"table-main__tournament\")\n",
    "            if header:\n",
    "                comp_slug = header[\"href\"].rstrip(\"/\")\n",
    "                # check if this is the correct league\n",
    "                in_target_comp = (competition_href in comp_slug)\n",
    "                continue\n",
    "\n",
    "            if not in_target_comp:\n",
    "                continue\n",
    "\n",
    "            # --- NORMAL MATCH ROW ---\n",
    "            cols = tr.find_all(\"td\")\n",
    "            if len(cols) < 5:\n",
    "                continue\n",
    "\n",
    "            # find score td\n",
    "            score_td = None\n",
    "            for td in cols:\n",
    "                if \":\" in td.get_text():\n",
    "                    score_td = td\n",
    "                    break\n",
    "\n",
    "            if not score_td:\n",
    "                continue\n",
    "\n",
    "            # extract score\n",
    "            m = re.search(r\"(\\d+)\\s*:\\s*(\\d+)\", score_td.get_text())\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            # determine home/away for THIS team\n",
    "            # The <strong> tag marks the current team\n",
    "            home_col = cols[2]\n",
    "            away_col = cols[3]\n",
    "\n",
    "            is_home = home_col.find(\"strong\") is not None\n",
    "\n",
    "            home_goals = int(m.group(1))\n",
    "            away_goals = int(m.group(2))\n",
    "\n",
    "            if is_home:\n",
    "                gf, ga = home_goals, away_goals\n",
    "                opponent = away_col.get_text(strip=True)\n",
    "            else:\n",
    "                gf, ga = away_goals, home_goals\n",
    "                opponent = home_col.get_text(strip=True)\n",
    "\n",
    "            results.append({\n",
    "                \"round\": cols[1].get_text(strip=True),\n",
    "                \"is_home\": is_home,\n",
    "                \"opponent\": opponent,\n",
    "                \"goals_for\": gf,\n",
    "                \"goals_against\": ga,\n",
    "                \"date\": cols[-1].get_text(strip=True),\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # 4) RUN FULL FLOW\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    final_list = []\n",
    "    for fixture_url in linkss:\n",
    "        \n",
    "        home_url, away_url, competition_href = get_team_links_and_comp(fixture_url[\"link\"])\n",
    "\n",
    "        # fetch /results/ pages\n",
    "        home_soup = get_team_results_page(home_url)\n",
    "        away_soup = get_team_results_page(away_url)\n",
    "\n",
    "        # extract filtered results\n",
    "        home_results = get_results_for_competition(home_soup, competition_href)\n",
    "        away_results = get_results_for_competition(away_soup, competition_href)\n",
    "\n",
    "        homes = []\n",
    "        aways = []\n",
    "\n",
    "        for r in home_results:\n",
    "            homes.append(r)\n",
    "\n",
    "        for r in away_results:\n",
    "            aways.append(r)\n",
    "\n",
    "        # append this fixture's data to the master list\n",
    "        final_list.append({\n",
    "            \"fixture_url\": fixture_url[\"link\"],\n",
    "            \"home_url\": home_url,\n",
    "            \"away_url\": away_url,\n",
    "            \"homes\": homes,\n",
    "            \"aways\": aways,\n",
    "            \"date\": fixture_url[\"date\"]\n",
    "        })\n",
    "    \n",
    "\n",
    "    def url_split(url):\n",
    "        slug = url.split(\"/team/\")[1].split(\"/\")[0]\n",
    "        team_name = slug.replace(\"-\", \" \").title()\n",
    "        return team_name\n",
    "\n",
    "    final_output = []\n",
    "    for item in final_list:\n",
    "        stats = compute_fixture_stats(\n",
    "            url_split(item[\"home_url\"]),\n",
    "            url_split(item[\"away_url\"]),\n",
    "            item[\"homes\"],\n",
    "            item[\"aways\"],\n",
    "            item[\"date\"],\n",
    "        )\n",
    "\n",
    "        final_output.append(stats)\n",
    "        \n",
    "\n",
    "    def save_output_to_files_template(final_output, csv_name=\"fixture_stats.csv\", excel_name=\"fixture_stats.xlsx\"):\n",
    "        \"\"\"\n",
    "        Saves final_output to CSV and Excel following the EXACT template order requested.\n",
    "        \"\"\"\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 1. FIXED TEMPLATE COLUMN ORDER (DO NOT CHANGE)\n",
    "        # -----------------------------------------------------------\n",
    "        columns = [\n",
    "            \"sorting_balance\",\n",
    "            \"home_team\",\n",
    "            \"away_team\",\n",
    "            \"date\",\n",
    "\n",
    "            \"HTS\", \"HTC\", \"ATS\", \"ATC\",\n",
    "\n",
    "            \"HS_plus_1\", \"HS-1\",\n",
    "            \"HC_plus_1\", \"HC-1\",\n",
    "            \"AS_plus_1\", \"AS-1\",\n",
    "            \"AC_plus_1\", \"AC-1\",\n",
    "\n",
    "            \"PHW\", \"PHX\", \"PHL\",\n",
    "            \"PAW\", \"PAX\", \"PAL\",\n",
    "        ]\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 2. FLATTEN final_output\n",
    "        # -----------------------------------------------------------\n",
    "        rows = []\n",
    "        for item in final_output:\n",
    "\n",
    "            stats = item.get(\"stats\", {})\n",
    "\n",
    "            row = {\n",
    "                \"sorting_balance\": item.get(\"sorting_balance\"),\n",
    "                \"home_team\": item.get(\"home_team\"),\n",
    "                \"away_team\": item.get(\"away_team\"),\n",
    "                \"date\": item.get(\"date\"),\n",
    "\n",
    "                \"HTS\": stats.get(\"HTS\"),\n",
    "                \"HTC\": stats.get(\"HTC\"),\n",
    "                \"ATS\": stats.get(\"ATS\"),\n",
    "                \"ATC\": stats.get(\"ATC\"),\n",
    "\n",
    "                \"HS_plus_1\": stats.get(\"HS_plus_1\"),\n",
    "                \"HS-1\": stats.get(\"HS-1\"),\n",
    "                \"HC_plus_1\": stats.get(\"HC_plus_1\"),\n",
    "                \"HC-1\": stats.get(\"HC-1\"),\n",
    "                \"AS_plus_1\": stats.get(\"AS_plus_1\"),\n",
    "                \"AS-1\": stats.get(\"AS-1\"),\n",
    "                \"AC_plus_1\": stats.get(\"AC_plus_1\"),\n",
    "                \"AC-1\": stats.get(\"AC-1\"),\n",
    "\n",
    "                \"PHW\": stats.get(\"PHW\"),\n",
    "                \"PHX\": stats.get(\"PHX\"),\n",
    "                \"PHL\": stats.get(\"PHL\"),\n",
    "                \"PAW\": stats.get(\"PAW\"),\n",
    "                \"PAX\": stats.get(\"PAX\"),\n",
    "                \"PAL\": stats.get(\"PAL\"),\n",
    "            }\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 3. WRITE CSV (Simple)\n",
    "        # -----------------------------------------------------------\n",
    "        with open(csv_name, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(rows)\n",
    "\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 4. CREATE BEAUTIFUL EXCEL\n",
    "        # -----------------------------------------------------------\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "\n",
    "        # Title\n",
    "        title = \"MATCH FIXTURE STATISTICS\"\n",
    "        ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=len(columns))\n",
    "        tcell = ws.cell(row=1, column=1)\n",
    "        tcell.value = title\n",
    "        tcell.font = Font(size=16, bold=True)\n",
    "        tcell.alignment = Alignment(horizontal=\"center\")\n",
    "        tcell.fill = PatternFill(start_color=\"D9E1F2\", fill_type=\"solid\")\n",
    "\n",
    "        # Header styling\n",
    "        header_fill = PatternFill(start_color=\"BDD7EE\", fill_type=\"solid\")\n",
    "        header_font = Font(bold=True)\n",
    "        border = Border(\n",
    "            left=Side(style=\"thin\"),\n",
    "            right=Side(style=\"thin\"),\n",
    "            top=Side(style=\"thin\"),\n",
    "            bottom=Side(style=\"thin\")\n",
    "        )\n",
    "\n",
    "        # Header row\n",
    "        for col_idx, col_name in enumerate(columns, start=1):\n",
    "            cell = ws.cell(row=2, column=col_idx, value=col_name)\n",
    "            cell.font = header_font\n",
    "            cell.fill = header_fill\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            cell.border = border\n",
    "\n",
    "        # Data rows with alternating colors\n",
    "        fill_white = PatternFill(start_color=\"FFFFFF\", fill_type=\"solid\")\n",
    "        fill_gray = PatternFill(start_color=\"F2F2F2\", fill_type=\"solid\")\n",
    "\n",
    "        for row_idx, row in enumerate(rows, start=3):\n",
    "            fill = fill_gray if row_idx % 2 == 0 else fill_white\n",
    "\n",
    "            for col_idx, col_name in enumerate(columns, start=1):\n",
    "                value = row.get(col_name, \"\")\n",
    "                cell = ws.cell(row=row_idx, column=col_idx, value=value)\n",
    "                cell.fill = fill\n",
    "                cell.border = border\n",
    "\n",
    "                if isinstance(value, (int, float)):\n",
    "                    cell.alignment = Alignment(horizontal=\"center\")\n",
    "                else:\n",
    "                    cell.alignment = Alignment(horizontal=\"left\")\n",
    "\n",
    "        # Auto-size columns\n",
    "        for col_idx in range(1, len(columns) + 1):\n",
    "            letter = get_column_letter(col_idx)\n",
    "            max_length = max(len(str(ws.cell(row=r, column=col_idx).value or \"\")) for r in range(1, len(rows) + 3))\n",
    "            ws.column_dimensions[letter].width = max_length + 2\n",
    "\n",
    "        wb.save(excel_name)\n",
    "        \n",
    "    return final_output\n",
    "\n",
    "\n",
    "def get_country_and_competition_from_url(url):\n",
    "    \"\"\"\n",
    "    Given a URL like:\n",
    "      https://www.betexplorer.com/football/england/premier-league/\n",
    "    Returns:\n",
    "      country = \"England\"\n",
    "      competition = \"Premier League\"\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path  # e.g. \"/football/england/premier-league/\"\n",
    "    parts = [p for p in path.split(\"/\") if p]  # split and remove empty\n",
    "\n",
    "    # parts should look like [\"football\", \"england\", \"premier-league\", ...]\n",
    "    country = None\n",
    "    competition = None\n",
    "\n",
    "    # We assume the structure /football/<country>/<competition>/\n",
    "    if len(parts) >= 3 and parts[0].lower() == \"football\":\n",
    "        country = parts[1].replace(\"-\", \" \").title()\n",
    "        competition = parts[2].replace(\"-\", \" \").title()\n",
    "    else:\n",
    "        # fallback: try last two parts\n",
    "        if len(parts) >= 2:\n",
    "            country = parts[-2].replace(\"-\", \" \").title()\n",
    "            competition = parts[-1].replace(\"-\", \" \").title()\n",
    "\n",
    "    return country, competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5dc28fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/football/nigeria/nnl/', '/football/europe/champions-league/', '/football/andorra/primera-divisio/', '/football/asia/afc-champions-league/', '/football/asia/afc-champions-league-2/', '/football/asia/arabian-gulf-cup-u23/', '/football/asia/southeast-asian-games-women/', '/football/australia/a-league-women/', '/football/azerbaijan/i-liqa/', '/football/belgium/national-division-1-vv/', '/football/bolivia/division-profesional/', '/football/brazil/copa-do-brasil-u20/', '/football/brazil/copinha-women/', '/football/ecuador/liga-pro/', '/football/egypt/league-cup/', '/football/england/championship/', '/football/england/league-one/', '/football/england/league-two/', '/football/england/national-league/', '/football/england/national-league-north/', '/football/england/npl-premier-division/', '/football/england/southern-league-premier-south/', '/football/england/premier-league-cup/', '/football/europe/champions-league/', '/football/europe/uefa-youth-league/', '/football/europe/premier-league-international-cup/', '/football/europe/champions-league-women/', '/football/france/coupe-de-la-ligue-women/', '/football/georgia/crystalbet-erovnuli-liga/', '/football/georgia/georgian-cup/', '/football/germany/regionalliga-nordost/', '/football/germany/bundesliga-women/', '/football/guatemala/liga-nacional/', '/football/iran/persian-gulf-pro-league/', '/football/italy/coppa-italia-serie-c/', '/football/jamaica/premier-league/', '/football/jordan/shield-cup/', '/football/malawi/super-league/', '/football/mali/premiere-division/', '/football/malta/challenge-league/', '/football/montenegro/prva-crnogorska-liga/', '/football/montenegro/druga-liga/', '/football/netherlands/eerste-divisie/', '/football/nigeria/nnl/', '/football/rwanda/premier-league/', '/football/san-marino/coppa-titano/', '/football/scotland/lowland-league/', '/football/scotland/scottish-cup/', '/football/sierra-leone/premier-league/', '/football/slovakia/slovak-cup/', '/football/spain/primera-rfef-group-2/', '/football/spain/segunda-rfef-group-4/', '/football/spain/tercera-rfef-group-15/', '/football/spain/tercera-rfef-group-18/', '/football/suriname/sml/', '/football/switzerland/challenge-league/', '/football/thailand/thai-league-2/', '/football/trinidad-and-tobago/tt-premier-league/', '/football/uganda/premier-league/', '/football/ukraine/championship-women/', '/football/uzbekistan/super-league/', '/football/wales/cymru-south/', '/football/wales/cymru-north/', '/football/world/fifa-intercontinental-cup/', '/football/world/club-friendly-2025/']\n"
     ]
    }
   ],
   "source": [
    "def extract_all_league_links():\n",
    "    url = BASE + \"/\"\n",
    "    html = requests.get(url, timeout=15).text\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    leagues = []\n",
    "\n",
    "    # Each league section looks like:\n",
    "    # <ul class=\"leagues-list\" data-country=\"england\">\n",
    "    league_boxes = soup.select(\"ul.leagues-list\")\n",
    "\n",
    "    for box in league_boxes:\n",
    "        country = box.get(\"data-country\", \"\").strip()\n",
    "\n",
    "        # Each league row has this structure:\n",
    "        # <a href=\"/football/england/premier-league/\" ...>\n",
    "        links = box.select(\"a.leaguesName500, a.leaguesName400, a.leaguesName350\")\n",
    "\n",
    "        for a in links:\n",
    "            href = a.get(\"href\", \"\").strip()\n",
    "\n",
    "            if not href.startswith(\"/football/\"):\n",
    "                continue  # skip ads or invalid links\n",
    "\n",
    "            # Extract league name (text inside <p>)\n",
    "            p = a.find(\"p\")\n",
    "            if not p:\n",
    "                continue\n",
    "\n",
    "            full_name = p.get_text(strip=True)  # e.g. \"England: Premier League\"\n",
    "\n",
    "            # Split into country and league\n",
    "            if \":\" in full_name:\n",
    "                ctry, league = [x.strip() for x in full_name.split(\":\", 1)]\n",
    "            else:\n",
    "                ctry = country.capitalize()\n",
    "                league = full_name\n",
    "\n",
    "            # Construct full URL\n",
    "            full_url = href\n",
    "\n",
    "            leagues.append({\n",
    "                \"country\": ctry,\n",
    "                \"league\": league,\n",
    "                \"url\": full_url\n",
    "            })\n",
    "\n",
    "    return leagues\n",
    "\n",
    "linkings = []\n",
    "# ------------------ TEST ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data = extract_all_league_links()\n",
    "    for league in data:\n",
    "        linkings.append(league['url'])\n",
    "\n",
    "print(linkings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384e2b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping League Link 1 of 1...\n",
      "League Link 1 of 1 Scraped!\n",
      "Everthing Scrapped Succesfully!\n"
     ]
    }
   ],
   "source": [
    "linkings2 = ['/football/england/premier-league/',]\n",
    "\n",
    "leagues = linkings2\n",
    "\n",
    "found = []\n",
    "count = 1\n",
    "for i in leagues:\n",
    "    print(f'Scrapping League Link {count} of {len(leagues)}...')\n",
    "    country, competition = get_country_and_competition_from_url(i)\n",
    "    leag = f'{country}-{competition}'\n",
    "    found.append({'comps': download(i), 'league': leag})\n",
    "    print(f'League Link {count} of {len(leagues)} Scraped!')\n",
    "    count = count + 1\n",
    "print('Everthing Scrapped Succesfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d776069",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "\n",
    "for block in found:   # your outer list\n",
    "    comps = block.get(\"comps\", [])\n",
    "    cleaned_comps = [c for c in comps if c is not None]\n",
    "\n",
    "    cleaned.append({\n",
    "        \"league\": block.get(\"league\"),\n",
    "        \"comps\": cleaned_comps\n",
    "    })\n",
    "\n",
    "found2 = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6db8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV and Excel files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def generate_files_from_output(final_output):\n",
    "    \"\"\"\n",
    "    Returns IN-MEMORY CSV and EXCEL files \n",
    "    with each match taking 2 rows merged vertically,\n",
    "    numeric data rounded to 2 decimals, and match info in a single wide cell.\n",
    "    Adds a LEAGUE NAME column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten dataset + attach league name\n",
    "    flat_matches = []\n",
    "    for block in final_output:\n",
    "        league_name = block.get(\"league\")\n",
    "        comps = block.get(\"comps\", [])\n",
    "        for match in comps:\n",
    "            match[\"league_name\"] = league_name\n",
    "            flat_matches.append(match)\n",
    "\n",
    "    columns = [\n",
    "        \"sorting_balance\",\n",
    "        \"match_info\",\n",
    "        \"league_name\",\n",
    "        \"HTS\", \"HTC\", \"ATS\", \"ATC\",\n",
    "        \"HS_plus_1\", \"HS-1\", \"HC_plus_1\", \"HC-1\",\n",
    "        \"AS_plus_1\", \"AS-1\", \"AC_plus_1\", \"AC-1\",\n",
    "        \"PHW\", \"PHX\", \"PHL\", \"PAW\", \"PAX\", \"PAL\",\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for item in flat_matches:\n",
    "        if item is None:\n",
    "            continue\n",
    "\n",
    "        stats = item.get(\"stats\", {})\n",
    "\n",
    "        match_info = f\"{item.get('home_team','')}   {item.get('date','')}   {item.get('away_team','')}\"\n",
    "\n",
    "        row = {\n",
    "            \"sorting_balance\": item.get(\"sorting_balance\"),\n",
    "            \"match_info\": match_info,\n",
    "            \"league_name\": item.get(\"league_name\"),\n",
    "        }\n",
    "\n",
    "        for key in [\"HTS\",\"HTC\",\"ATS\",\"ATC\",\n",
    "                    \"HS_plus_1\",\"HS-1\",\"HC_plus_1\",\"HC-1\",\n",
    "                    \"AS_plus_1\",\"AS-1\",\"AC_plus_1\",\"AC-1\",\n",
    "                    \"PHW\",\"PHX\",\"PHL\",\"PAW\",\"PAX\",\"PAL\"]:\n",
    "            val = stats.get(key)\n",
    "            if val is not None:\n",
    "                val = round(val, 2)\n",
    "            row[key] = val\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    # ----------------- CSV IN MEMORY -----------------\n",
    "    csv_buffer = StringIO()\n",
    "    writer = csv.DictWriter(csv_buffer, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "    csv_data = csv_buffer.getvalue()\n",
    "\n",
    "    # ----------------- EXCEL IN MEMORY -----------------\n",
    "    excel_buffer = BytesIO()\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    # TITLE\n",
    "    total_cols = len(columns)\n",
    "    ws.merge_cells(start_row=1, start_column=1, end_row=1, end_column=total_cols)\n",
    "    tcell = ws.cell(row=1, column=1, value=\"MATCH FIXTURE STATISTICS\")\n",
    "    tcell.font = Font(size=18, bold=True)\n",
    "    tcell.alignment = Alignment(horizontal=\"center\")\n",
    "    tcell.fill = PatternFill(start_color=\"D9E1F2\", fill_type=\"solid\")\n",
    "\n",
    "    # HEADER\n",
    "    header_fill = PatternFill(start_color=\"BDD7EE\", fill_type=\"solid\")\n",
    "    header_font = Font(bold=True)\n",
    "\n",
    "    for col_idx, col_name in enumerate(columns, start=1):\n",
    "        cell = ws.cell(row=2, column=col_idx, value=col_name)\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = Alignment(horizontal=\"center\")\n",
    "\n",
    "    # DATA (merge every two rows)\n",
    "    start_row = 3\n",
    "    for row in rows:\n",
    "        for col_idx, col in enumerate(columns, start=1):\n",
    "            ws.merge_cells(start_row=start_row, start_column=col_idx, end_row=start_row+1, end_column=col_idx)\n",
    "            cell = ws.cell(row=start_row, column=col_idx, value=row.get(col))\n",
    "            cell.alignment = Alignment(horizontal=\"center\")\n",
    "        start_row += 2\n",
    "\n",
    "    # COLUMN WIDTHS\n",
    "    for col_idx, col in enumerate(columns, start=1):\n",
    "        letter = get_column_letter(col_idx)\n",
    "\n",
    "        if col == \"match_info\":\n",
    "            ws.column_dimensions[letter].width = 50\n",
    "        elif col == \"league_name\":\n",
    "            ws.column_dimensions[letter].width = 25\n",
    "        else:\n",
    "            ws.column_dimensions[letter].width = 18\n",
    "\n",
    "    wb.save(excel_buffer)\n",
    "    excel_buffer.seek(0)\n",
    "    \n",
    "    return csv_data, excel_buffer\n",
    "\n",
    "\n",
    "csv_data, excel_buffer = generate_files_from_output(found2)\n",
    "\n",
    "# Save CSV\n",
    "with open(\"fixtures.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Save Excel\n",
    "with open(\"fixtures.xlsx\", \"wb\") as f:\n",
    "    f.write(excel_buffer.getvalue())\n",
    "\n",
    "print(\"CSV and Excel files saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557d677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
